{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee38dcda-e45b-44dc-be80-4519da755add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: torch in /databricks/python3/lib/python3.9/site-packages (1.12.1+cu113)\nCollecting torch\n  Downloading torch-2.7.0-cp39-cp39-manylinux_2_28_x86_64.whl (865.2 MB)\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.9/site-packages (4.21.2)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\nCollecting peft\n  Downloading peft-0.15.2-py3-none-any.whl (411 kB)\nCollecting accelerate\n  Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\nCollecting datasets\n  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\nCollecting nvidia-nvjitlink-cu12==12.6.85\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.9/site-packages (from torch) (2.6.3)\nCollecting nvidia-curand-cu12==10.3.7.77\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\nCollecting typing-extensions>=4.10.0\n  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\nCollecting nvidia-nccl-cu12==2.26.2\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\nCollecting nvidia-nvtx-cu12==12.6.77\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\nCollecting nvidia-cublas-cu12==12.6.4.1\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\nCollecting nvidia-cusparselt-cu12==0.6.3\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\nCollecting nvidia-cudnn-cu12==9.5.1.17\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.9/site-packages (from torch) (3.3.1)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.9/site-packages (from torch) (2021.8.1)\nCollecting nvidia-cufile-cu12==1.11.1.6\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\nCollecting triton==3.3.0\n  Downloading triton-3.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\nCollecting sympy>=1.13.3\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\nCollecting nvidia-cusparse-cu12==12.5.4.2\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from torch) (2.11.3)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.9/dist-packages (from triton==3.3.0->torch) (58.0.4)\nCollecting tokenizers<0.22,>=0.21\n  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.9/site-packages (from transformers) (2021.8.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from transformers) (24.2)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.9/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.9/site-packages (from transformers) (4.62.3)\nCollecting huggingface-hub<1.0,>=0.30.0\n  Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\nCollecting safetensors>=0.4.3\n  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.9/site-packages (from transformers) (6.0)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.9/site-packages (from peft) (5.8.0)\nCollecting multiprocess<0.70.17\n  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\nCollecting xxhash\n  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from datasets) (1.3.4)\nCollecting fsspec[http]<=2025.3.0,>=2023.1.0\n  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\nCollecting pyarrow>=15.0.0\n  Downloading pyarrow-20.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (42.3 MB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /databricks/python3/lib/python3.9/site-packages (from datasets) (0.3.4)\nCollecting requests\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\nCollecting tqdm>=4.27\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n  Downloading aiohttp-3.11.18-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (21.2.0)\nCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (335 kB)\nCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\nCollecting hf-xet<2.0.0,>=1.1.0\n  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (3.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\nInstalling collected packages: typing-extensions, propcache, nvidia-nvjitlink-cu12, multidict, frozenlist, yarl, tqdm, requests, nvidia-cusparse-cu12, nvidia-cublas-cu12, mpmath, hf-xet, fsspec, async-timeout, aiosignal, aiohappyeyeballs, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparselt-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, huggingface-hub, dill, aiohttp, xxhash, torch, tokenizers, safetensors, pyarrow, multiprocess, transformers, datasets, accelerate, peft, evaluate\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 3.10.0.2\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.62.3\n    Not uninstalling tqdm at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'tqdm'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.26.0\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2021.8.1\n    Not uninstalling fsspec at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'fsspec'. No files were found to uninstall.\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.9.1\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.4\n    Not uninstalling dill at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'dill'. No files were found to uninstall.\n  Attempting uninstall: torch\n    Found existing installation: torch 1.12.1+cu113\n    Not uninstalling torch at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'torch'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 7.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.21.2\n    Not uninstalling transformers at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc\n    Can't uninstall 'transformers'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.11.4 requires pyspark>=2.1.0, which is not installed.\ntorchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 2.7.0 which is incompatible.\nmlflow-skinny 1.29.0 requires packaging<22, but you have packaging 24.2 which is incompatible.\nSuccessfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.6.0 fsspec-2025.3.0 hf-xet-1.1.0 huggingface-hub-0.31.1 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.16 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 peft-0.15.2 propcache-0.3.1 pyarrow-20.0.0 requests-2.32.3 safetensors-0.5.3 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.51.3 triton-3.3.0 typing-extensions-4.13.2 xxhash-3.5.0 yarl-1.20.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting py7zr\n  Downloading py7zr-0.22.0-py3-none-any.whl (67 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\nCollecting entmax\n  Downloading entmax-1.3-py3-none-any.whl (13 kB)\nCollecting pyppmd<1.2.0,>=1.1.0\n  Downloading pyppmd-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.9/site-packages (from py7zr) (5.8.0)\nCollecting pycryptodomex>=3.16.0\n  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\nCollecting inflate64<1.1.0,>=1.0.0\n  Downloading inflate64-1.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\nCollecting brotli>=1.1.0\n  Downloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\nCollecting multivolumefile>=0.2.3\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting pyzstd>=0.15.9\n  Downloading pyzstd-0.16.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\nCollecting texttable\n  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\nCollecting pybcj<1.1.0,>=1.0.0\n  Downloading pybcj-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\nRequirement already satisfied: absl-py in /databricks/python3/lib/python3.9/site-packages (from rouge_score) (1.0.0)\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.9/site-packages (from rouge_score) (3.6.5)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from rouge_score) (1.20.3)\nRequirement already satisfied: six>=1.14.0 in /databricks/python3/lib/python3.9/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: torch>=1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from entmax) (2.7.0)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (11.3.0.4)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (12.6.85)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (12.6.80)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (12.6.77)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.9/site-packages (from torch>=1.3->entmax) (2.6.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (10.3.7.77)\nRequirement already satisfied: typing-extensions>=4.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (4.13.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (12.6.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (11.7.1.2)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (12.6.4.1)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (0.6.3)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (9.5.1.17)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.9/site-packages (from torch>=1.3->entmax) (3.3.1)\nRequirement already satisfied: fsspec in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (2025.3.0)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (1.11.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (12.6.77)\nRequirement already satisfied: triton==3.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (3.3.0)\nRequirement already satisfied: sympy>=1.13.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (1.14.0)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from torch>=1.3->entmax) (12.5.4.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from torch>=1.3->entmax) (2.11.3)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.9/dist-packages (from triton==3.3.0->torch>=1.3->entmax) (58.0.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=1.3->entmax) (1.3.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2->torch>=1.3->entmax) (2.0.1)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.9/site-packages (from nltk->rouge_score) (2021.8.3)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk->rouge_score) (1.0.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py): started\n  Building wheel for rouge-score (setup.py): finished with status 'done'\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=ad68825d52c0fc3ed07223dd32779569892d3017895b98f0ef04988f32642823\n  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\nSuccessfully built rouge-score\nInstalling collected packages: texttable, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, brotli, rouge-score, py7zr, entmax\nSuccessfully installed brotli-1.1.0 entmax-1.3 inflate64-1.0.1 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.6 pycryptodomex-3.22.0 pyppmd-1.1.1 pyzstd-0.16.2 rouge-score-0.1.2 texttable-1.7.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch transformers peft accelerate datasets evaluate\n",
    "%pip install py7zr rouge_score entmax\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d2a68f-4ab4-4339-acef-60524a604941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79c93c9-919b-476c-b341-ae58e926567e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model config\n",
    "top_k = 1\n",
    "router_type = \"attention\"\n",
    "norm_type   = \"softmax\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc0afc3-5a2a-4d30-9f21-307419192e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n  warn(f\"Failed to load image Python extension: {e}\")\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e7256caa5046c286bc21e686678925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3af14ad1b0423db292d1880484e399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "samsum.py:   0%|          | 0.00/3.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum.\n",
       "You can avoid this prom... yes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ffdb2e59014c039915c97cb2a1a403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.7z:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681b1debedd244e7bc0b6328476dcbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d3105470414c289f7e6d2650914f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04968d089024f1a93c90862622c39d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3dd760417f49988819b1d642148524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6823cf5b47144406ae20ccc58b686228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095ba6da58b548c2a96674bfa3afc522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55c4528af6f422192644a99b48be03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9283b53a52404e04bfb0d01e3020729d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c5b45e4bf74e218773897a692c1de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9b2779fe84461f9ebd36ebbe29151b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-8a1f4d91-7fb2-473d-ab87-ddc170b6ecdc/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450699cad9ec42c39cf87f06e76ffab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb0f68b458442c1abaa88b0c6ef733f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a2771e7954417a859d675ba9bbaff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed7033967114e339683684fbb8e9444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce9be956b27438c9fa2b6697b36efb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All routers replaced\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc3aa86ca184b4ab3f691c4da940cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<command-8808008584040257>:260: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n\n🧪  Eval before training (first 50 examples):\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.294771194458008, 'eval_model_preparation_time': 0.0117, 'eval_rouge1': 1.6174, 'eval_rouge2': 0.0, 'eval_rougeL': 1.6067, 'eval_rougeLsum': 1.5752, 'eval_runtime': 11.1679, 'eval_samples_per_second': 4.477, 'eval_steps_per_second': 1.164}\n\n🚀  Start fine-tuning …\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- 1.  imports ---------------------------\n",
    "import evaluate, nltk, torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    SwitchTransformersForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2.  Sparsemax + attention router\n",
    "# -------------------------------------------------------------------\n",
    "class SoftmaxNorm(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(x, dim=self.dim)\n",
    "    \n",
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - x.mean(dim=self.dim, keepdim=True)\n",
    "        z = torch.clamp(x, min=0)                               # ReLU\n",
    "        z_sorted, _ = torch.sort(z, dim=self.dim, descending=True)\n",
    "        z_cumsum = z_sorted.cumsum(dim=self.dim)\n",
    "        rhos = torch.arange(1, z.size(self.dim) + 1,\n",
    "                            device=x.device, dtype=x.dtype)\n",
    "        condition = z_sorted * rhos > (z_cumsum - 1)\n",
    "        rho = condition.sum(dim=self.dim, keepdim=True)\n",
    "        tau = (z_cumsum.gather(self.dim, rho - 1) - 1) / rho\n",
    "        return torch.clamp(z - tau, min=0)\n",
    "    \n",
    "class LinearRouter(nn.Module):\n",
    "    def __init__(self, config, norm: nn.Module, top_k: int = None):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_size, config.num_experts)\n",
    "        self.norm = norm\n",
    "        self.top_k = top_k if top_k is not None else config.num_experts\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logits = self.gate(hidden_states)                # [B,T,E]\n",
    "        probs = self.norm(logits)                        # [B,T,E]\n",
    "        \n",
    "        if self.top_k < probs.size(-1):\n",
    "            # Select top-k probabilities and zero out the rest\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, k=self.top_k, dim=-1)  # [B,T,k]\n",
    "            # Create a zero tensor with the same shape as probs\n",
    "            sparse_probs = torch.zeros_like(probs)  # [B,T,E]\n",
    "            # Scatter top-k probabilities back to their original indices\n",
    "            sparse_probs.scatter_(-1, top_k_indices, top_k_probs)\n",
    "            probs = sparse_probs\n",
    "\n",
    "        z_loss = torch.logsumexp(logits, -1).pow(2).mean()\n",
    "        return probs, z_loss\n",
    "\n",
    "class AttentionRouter(nn.Module):\n",
    "    def __init__(self, config, norm: nn.Module, top_k: int = None):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.expert_keys = nn.Parameter(torch.randn(config.num_experts, config.hidden_size))\n",
    "        nn.init.normal_(self.expert_keys, mean=0., std=0.02)\n",
    "        self.norm = norm\n",
    "        self.top_k = top_k if top_k is not None else config.num_experts\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        B, T, H = hidden_states.shape\n",
    "\n",
    "        # Compute queries and keys\n",
    "        q = self.query(hidden_states).view(-1, H)  # [B*T, H]\n",
    "        k = self.key(self.expert_keys)  # [E, H]\n",
    "    \n",
    "        # Compute logits using the transformed keys\n",
    "        logits = (q @ k.T).view(B, T, -1)  # [B,T,E]\n",
    "        probs = self.norm(logits)  # [B,T,E]\n",
    "        \n",
    "        if self.top_k < probs.size(-1):\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, k=self.top_k, dim=-1)  # [B,T,k]\n",
    "            sparse_probs = torch.zeros_like(probs)  # [B,T,E]\n",
    "            sparse_probs.scatter_(-1, top_k_indices, top_k_probs)\n",
    "            probs = sparse_probs\n",
    "\n",
    "        z_loss = torch.logsumexp(logits, -1).pow(2).mean()\n",
    "        return probs, z_loss\n",
    "    \n",
    "def make_router(config, top_k: int = None):\n",
    "    norm = SoftmaxNorm(dim=-1) if norm_type == \"softmax\" else Sparsemax(dim=-1)\n",
    "    if router_type == \"linear\":\n",
    "        return LinearRouter(config, norm, top_k=top_k)\n",
    "    elif router_type == \"attention\":\n",
    "        return AttentionRouter(config, norm, top_k=top_k)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown router_type={router_type}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3.  Monkey-patch the MoE layer: full sparse weighted sum\n",
    "# -------------------------------------------------------------------\n",
    "from transformers.models.switch_transformers import modeling_switch_transformers as mst\n",
    "\n",
    "def sparsemlp_forward(self, hidden_states):\n",
    "    \"\"\"\n",
    "    Fully-sparse mix of expert outputs using Sparsemax router probabilities.\n",
    "    Returns (combined_hidden_states,\n",
    "             (router_logits, dummy_expert_indices))\n",
    "    so the HuggingFace unpacker is satisfied.\n",
    "    \"\"\"\n",
    "    # 1) run every expert\n",
    "    if isinstance(self.experts, nn.ModuleDict):\n",
    "        expert_outputs = [m(hidden_states) for m in self.experts.values()]\n",
    "    else:                                            # ModuleList\n",
    "        expert_outputs = [m(hidden_states) for m in self.experts]\n",
    "    expert_outputs = torch.stack(expert_outputs, dim=2)   # [B,T,E,H]\n",
    "\n",
    "    # 2) routing\n",
    "    router_probs, router_z_loss = self.router(hidden_states)   # [B,T,E]\n",
    "\n",
    "    # 3) weighted mixture\n",
    "    combined = (expert_outputs * router_probs.unsqueeze(-1)).sum(dim=2)\n",
    "\n",
    "    # 4) create dummy expert-index tensor so shape rules match\n",
    "    dummy_idx = torch.zeros_like(router_probs[..., 0], dtype=torch.long)\n",
    "\n",
    "    # 5) return in the format HF expects\n",
    "    return combined, (router_probs, dummy_idx)\n",
    "\n",
    "# Replace the original forward\n",
    "mst.SwitchTransformersSparseMLP.forward = sparsemlp_forward\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4.  NLTK data & data set\n",
    "# -------------------------------------------------------------------\n",
    "nltk.download(\"punkt\")\n",
    "model_id   = \"google/switch-base-8\"\n",
    "dataset_id = \"samsum\"\n",
    "\n",
    "raw = load_dataset(dataset_id)\n",
    "tok  = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# determine max lengths\n",
    "tmp = concatenate_datasets([raw[\"train\"], raw[\"test\"]])\n",
    "max_src = max(len(t) for t in tmp.map(lambda x: tok(x[\"dialogue\"],\n",
    "                                                   truncation=True),\n",
    "                                      batched=True)[\"input_ids\"])\n",
    "max_tgt = max(len(t) for t in tmp.map(lambda x: tok(x[\"summary\"],\n",
    "                                                   truncation=True),\n",
    "                                      batched=True)[\"input_ids\"])\n",
    "\n",
    "def preprocess(batch, padding=\"max_length\"):\n",
    "    inputs = [\"summarize: \" + d for d in batch[\"dialogue\"]]\n",
    "    model_in = tok(inputs, max_length=max_src, truncation=True,\n",
    "                   padding=padding)\n",
    "    with tok.as_target_tokenizer():\n",
    "        labels = tok(batch[\"summary\"], max_length=max_tgt,\n",
    "                     truncation=True, padding=padding)[\"input_ids\"]\n",
    "    labels = [[t if t != tok.pad_token_id else -100 for t in lab]\n",
    "              for lab in labels]\n",
    "    model_in[\"labels\"] = labels\n",
    "    return model_in\n",
    "\n",
    "data = raw.map(preprocess, batched=True,\n",
    "               remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5.  Load model & replace all routers with our Sparsemax router\n",
    "# -------------------------------------------------------------------\n",
    "model = SwitchTransformersForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "# print(\"Pretrained model:\")\n",
    "# print(model)\n",
    "def replace_routers(model, top_k: int = None):\n",
    "    from transformers.models.switch_transformers import modeling_switch_transformers as mst\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, mst.SwitchTransformersSparseMLP):\n",
    "            module.router = make_router(model.config, top_k=top_k)\n",
    "replace_routers(model)\n",
    "print(\"✅ All routers replaced\")\n",
    "# print(\"New model:\")\n",
    "# print(model)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6.  Trainer setup\n",
    "# -------------------------------------------------------------------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "def postproc(preds, refs):\n",
    "    preds = [\"\\n\".join(sent_tokenize(p.strip())) for p in preds]\n",
    "    refs  = [\"\\n\".join(sent_tokenize(r.strip())) for r in refs]\n",
    "    return preds, refs\n",
    "\n",
    "def metrics(eval_pred):\n",
    "    preds, lab = eval_pred\n",
    "    if isinstance(preds, tuple): preds = preds[0]\n",
    "    preds = np.where(preds != -100, preds, tok.pad_token_id)\n",
    "    dec_preds = tok.batch_decode(preds, skip_special_tokens=True)\n",
    "    lab = np.where(lab != -100, lab, tok.pad_token_id)\n",
    "    dec_lab = tok.batch_decode(lab, skip_special_tokens=True)\n",
    "    dec_preds, dec_lab = postproc(dec_preds, dec_lab)\n",
    "    res = rouge.compute(predictions=dec_preds, references=dec_lab,\n",
    "                        use_stemmer=True)\n",
    "    return {k: round(v*100,4) for k,v in res.items()}\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tok, model, label_pad_token_id=-100,\n",
    "                                  pad_to_multiple_of=4)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"switch8\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    logging_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    compute_metrics=metrics,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. Training\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n🧪  Eval before training (first 50 examples):\")\n",
    "print(trainer.evaluate(eval_dataset=data[\"validation\"].select(range(50))))\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n🚀  Start fine-tuning …\")\n",
    "print(trainer.train())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. Saving and evaluate\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Save the model\n",
    "save_dir = f\"/dbfs/switch_base_8/{router_type}_{norm_type}_top{top_k}\"\n",
    "trainer.save_model(save_dir)\n",
    "print(f\"Model saved to {save_dir}\")\n",
    "\n",
    "# Print GPU memory usage\n",
    "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_exp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}